{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/rakshitviswanatham/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/rakshitviswanatham/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n",
      "writing to file\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "#loading data from text files, rename as per the file that needs to be processed.\n",
    "data = open(\"pg345.txt\", \"r\")\n",
    "data = data.read()\n",
    "data = data.encode().decode(\"utf-8\").lower()\n",
    "\n",
    "#Tokenizing the text\n",
    "tokens = [word for sent in nltk.sent_tokenize(data) for word in nltk.word_tokenize(sent)]\n",
    "\n",
    "#removing stopwords\n",
    "stop = stopwords.words('english')\n",
    "tokens = [token for token in tokens if token not in stop]\n",
    "\n",
    "#remvoing additional bunch of stopwords\n",
    "stopwords = open(\"stopwords_clean.txt\", \"r\")\n",
    "stopwords = stopwords.read()\n",
    "stopwords = stopwords.split(\"\\n\")\n",
    "\n",
    "clean_tokens = [token for token in tokens if token not in stopwords]\n",
    "\n",
    "#stemming the clean text using WordNetLemmatizer\n",
    "stemmed_text = list()\n",
    "clean_text = \"\"\n",
    "\n",
    "for word in clean_tokens:\n",
    "    temp_text = \"\".join(wnl.lemmatize(word))\n",
    "    stemmed_text.append(temp_text)\n",
    "#cleaning the stemmed text to remove everything but alphabets.\n",
    "stemmed_text = pd.Series(stemmed_text).str.replace(\"[^a-zA-Z]\", \"\")\n",
    "count = 0\n",
    "fileToWrite = \"\"\n",
    "fileCount = 0\n",
    "for new_word in stemmed_text:\n",
    "    #removing redundant stemmed text.\n",
    "    if new_word not in stopwords and len(new_word) > 2:\n",
    "        fileToWrite += new_word + \" \"\n",
    "        count += 1\n",
    "        #for every 50 words, breaking line to make paragraphs\n",
    "        if count % 50 == 0:\n",
    "            fileToWrite += '\\n'\n",
    "        #storing to file, clean 1500 words\n",
    "        if count == 1000:\n",
    "            print(\"writing to file\")\n",
    "            with open(\"test_\"+str(fileCount)+\".txt\", \"w\") as f:\n",
    "                f.write(fileToWrite)\n",
    "                f.close()\n",
    "                fileCount += 1\n",
    "            fileToWrite = \"\"\n",
    "            count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
